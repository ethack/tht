[{"content":"Threat Hunting Toolkit You have data. Now what? Don’t panic! You have many tools and methods at your disposal. This documentation will show how you can use and combine them to make your data more valuable.\n Explore  The simplest thing you can do is explore your raw data. There may be too much at first to be meaningful, but viewing your data is necessary to determine how to transform and summarize it. In data science this is called Exploratory Data Analysis (EDA).  Example: Using cat to view a log file or loading a CSV into a spreadsheet.   Visualizing your data through graphs and charts is a useful way to discover patterns and trends, identify outliers, and view relationships.   Filter  Reducing the amount of data through filtering is nearly always going to be your first step and often between other operations as well. Not only does this remove noise irrelevant to your question or goal, but it decreases the time it takes to process the remaining data. How to: Countless tools accomplish this in different ways. Anything that uses regular expressions, search terms, or comparisions is a way of filtering data. Common Linux utils like grep and awk, THT’s filter, your SIEM’s search bar, or even a SQL WHERE clause are all ways to accomplish this.   Compute  This is when you use fields from your existing data and perform some operation to create or derive a new field, sometimes referred to as a computed field. A simple example would be adding the bytes sent and received together to get the total bytes transferred. In data science this is known as Feature Engineering. Computing normally involves some custom programming, either in a formal language like Python, or in a domain specific language (DSL) in a tool like Miller.   Correlate  Closely related to pivoting, which is when you would manually use a piece of information to pivot into a different dataset. This can also be thought of as a join. You can correlate your data with other data by joining datasets together on common fields.  Example: Taking multiple log types that share a field (e.g. IP address) and joining them together to create a single entry containing data from both logs.   You can also enrich your data using data from other sources, such as an API or existing database.  Examples: Name resolution or passive DNS, geographical info, WHOIS or ownership info, or even threat intelligence feeds.     Summarize  Summarizing, also known as grouping, aggregating, or data stacking, reduces your data by combining data through methods such as the average, sum, or count.    Installing THT The Threat Hunting Toolkit (THT) is the name of the project, a docker image, as well as a wrapper script for launching. While you can use the docker image manually, the recommended way is through the wrapper script.\nThis will install the tht script in /usr/local/bin/tht.\nsudo curl -o /usr/local/bin/tht https://raw.githubusercontent.com/ethack/tht/main/tht \u0026\u0026 sudo chmod +x /usr/local/bin/tht Running THT This is the simplest way to launch THT.\ntht This will give you a zsh shell inside a new THT container. All the tools and examples from this documentation can now be used.\nTipYour host’s filesystem is accessible from /host.\n Advanced Usage With tht you can also run one-off commands or even give it scripts to execute within the context of the container. This is useful if you want to automate or schedule certain tasks from the host system.\nThe basic usage is tht run \u003ccommand\u003e.\ntht run \"echo hello world\" Result:\nhello world  This will run existing script.\ncat my_script.sh | tht run You can also specify multiple commands or an entire script like this.\ntht run \u003c\u003c\\SCRIPT message=GREAT! echo -n \"Running multiple commands \" echo -n \"without escaping feels $message \" echo {1..3} SCRIPT Result:\nRunning multiple commands without escaping feels GREAT! 1 2 3  tht run \u003c\u003c\\SCRIPT #!/usr/bin/env python3 print('Here is an example python program') SCRIPT Result:\nHere is an example python program  You might want to put a script like this in your host’s cron scheduler /etc/cron.hourly/pdns.\n#!/bin/bash  tht run \u003c\u003c\\SCRIPT cd /host/opt/zeek/logs/ nice flock -n \"/host/tmp/pdns.lock\" \\ fd 'dns.*log' | sort | xargs -n 24 bro-pdns index SCRIPT See the cron/ directory in the code repo for more examples of cron scripts.\nUpdating THT This will pull the latest image as well as latest tht script.\ntht update Result:\nDownloading latest THT image... Using default tag: latest latest: Pulling from ethack/tht Digest: sha256:ef98d36e379fb0f2d9537c39b9e53fcb8f349e2cbde9d9d37eb15d4299e0ac41 Status: Image is up to date for ethack/tht:latest docker.io/ethack/tht:latest Self-updating THT script...  Advantages Simplicity Here is an example of the power of THT. Let’s say your goal is to find a trend or anomaly in traffic to cloudfront.net.\nThe following command searches Zeek ssl.log files (compressed or not) for all events going to cloudfront.net. It then pulls out the timestamp, converts it to a date, and counts the frequency of events per day. Finally, it displays a bar graph of the result.\nfilter --ssl cloudfront.net | chop ts | ts2 date | freq | plot-bar Result:\n ┌────────────────────────────────────────────────────────────────────────────────────────────┐ 475620┤ █████████████▌ │ │ █████████████▌ │ │ █████████████▌ │ │ █████████████▌ │ 396350┤ █████████████▌ │ │ █████████████▌ │ │ █████████████▌ │ │ █████████████▌ │ 317080┤ █████████████▌ │ │ █████████████▌ │ │ █████████████▌ │ │ █████████████▌ │ 237810┤ █████████████▌ │ │ █████████████▌ ▄▄▄▄▄▄▄▄▄▄▄▄▄│ │ █████████████▌ █████████████│ │ █████████████▌ █████████████│ │ █████████████▌ █████████████│ 158540┤ █████████████▌ █████████████│ │ █████████████▌ █████████████│ │ ▐████████████▌ █████████████▌ █████████████│ │ ▐████████████▌ █████████████▌ █████████████│ 79270┤ ▐████████████▌ █████████████▌ █████████████│ │ ▐████████████▌ █████████████▌ █████████████│ │ ▐████████████▌ █████████████▌ █████████████│ │ ▗▄▄▄▄▄▄▄▄▄▄▄▄▖ ▐████████████▌ █████████████▌ █████████████│ 0┤▄▄▄▄▄▄▄▄▄▄▄▄▄ ▗▄▄▄▄▄▄▄▄▄▄▄▄▄ ▐████████████▌ ▐████████████▌ █████████████▌ █████████████│ └──────┬───────────────┬───────────────┬──────────────┬───────────────┬───────────────┬──────┘ 2021-06-21 2021-06-22 2021-06-23 2021-06-24 2021-06-25 2021-06-26 [y] Count [x]  Compare this to a (rougly) equivalent command without THT. It’s doable, but you have to be fluent in quite a few builtin Linux tools, as well as their various flags, and how to escape special characters. After all that you get the same information, but you have to compare relative size of numbers in the text output rather than looking at an graph.\nzgrep -hF cloudfront.net */ssl* | cut -d$'\\t' -f1 | sed 's/^/@/' | date -Idate -f - | sort -nr | uniq -c Result:\n 2910 2021-06-21 3326 2021-06-22 19308 2021-06-23 126939 2021-06-24 475620 2021-06-25 226890 2021-06-26  Speed Not only do the tools included in THT remove much of the arcane syntax and boilerplate associated with log parsing, but they are also generally faster.\nIn the above example, the THT version took 10.8 seconds.\nfilter --ssl cloudfront.net 64.85s user 5.54s system 670% cpu 10.491 total chop ts 2.56s user 2.10s system 44% cpu 10.492 total ts2 date 4.54s user 0.11s system 44% cpu 10.496 total sort --version-sort --buffer-size=2G 1.26s user 0.11s system 12% cpu 10.809 total uniq -c 0.05s user 0.01s system 0% cpu 10.808 total plot-bar 0.07s user 0.02s system 0% cpu 10.871 total  And the non-THT version took nearly 3x longer at 31.3 seconds.\nzgrep -hF cloudfront.net */ssl* 33.00s user 3.68s system 117% cpu 31.188 total cut -d$'\\t' -f1 1.41s user 0.48s system 6% cpu 31.187 total sed 's/^/@/' 0.32s user 0.04s system 1% cpu 31.187 total date -Idate -f - 1.23s user 0.04s system 4% cpu 31.186 total sort -n 0.56s user 0.05s system 1% cpu 31.306 total uniq -c 0.06s user 0.00s system 0% cpu 31.305 total  Complementary Projects These are all projects that work well together with THT.\n Elastic / Kibana Metabase ml-workspace Data Science at the Command Line  Further Resources  https://github.com/dbohdan/structured-text-tools  ","description":"","tags":null,"title":"Introduction","uri":"/tht/"},{"content":"Here is an overview of the pages in this category:\nChop Like `cut` but with column names and CSV/TSV/JSON/Zeek support without the boilerplate. Filter Parallel searching for IPs, domains, or regexes across [un]compressed logs. Plus special features for Zeek logs.   ug - Ugrep rg - Ripgrep grepcidr json-cut - Like zeek-cut but for JSON logs.  ","description":"","tags":null,"title":"Filter","uri":"/tht/filter/"},{"content":"Here is an overview of the pages in this category:\nConn Summary Output a summary of connections given an input of Zeek logs. Great for getting an overview of a subset of logs.   distinct - Outputs unique lines. count - Counts number of lines. card - Counts unique lines, printing the cardinality of the data. freq - Displays the number of occurrences, or frequency, of each unique line. mfo - Most Frequent Occurrence. Like freq but sorted by most commonly occurring unique line. Optionally pass a number to truncate results. lfo - Least Frequent Occurrence. Like freq but sorted by least commonly occurring unique line. Optionally pass a number to truncate results.  ","description":"","tags":null,"title":"Summarize","uri":"/tht/summarize/"},{"content":"Here is an overview of the pages in this category:\n zannotate dog mtr whois whois-bulk  ","description":"","tags":null,"title":"Correlate","uri":"/tht/correlate/"},{"content":"Here is an overview of the pages in this category:\n bat - Like cat but with line numbers, syntax highlighting, and scrolling. pxl - Display images in the terminal. Useful if you generate a chart and save it as an image. pv - Show a progress bar for long-running commands. plot-bar - Displays a bar chart in the terminal. zv, cv, tv - Zeek viewer, CSV viewer, TSV viewer will give you a scrollable table view of your data. zvt, cvt, tvt - Same as above, but outputs a copy-pastable ascii table. trim - Truncates output to fit in the terminal and prevent line wrapping (not scrollable). Usefel for copy-pasting.  ","description":"","tags":null,"title":"Visualize","uri":"/tht/visualize/"},{"content":"Here is an overview of the pages in this category:\n  miller xsv tsv-utils zq jq  ","description":"","tags":null,"title":"Multipurpose","uri":"/tht/multipurpose/"},{"content":"Here is an overview of the pages in this category:\n domain - Will truncate a full domain to a given number of subdomains (default 2). ts2 - Convert timestamps between different formats. dust - File size analyzer. entr - File change watcher. fd - File finder. Like find but better. fzf - Fuzzy finder. htop - System resource viewer. Like top but prettier. hyperfine - Benchmarking tool. Like time but more powerful. nq - Enqueue commands to run in the background. body cols header dseq ipcalc chronic combine ifne pee sponge zrun  Editors  nano - For those who have to have it. micro - For nano users who want better while still staying simple. vim - For emacs users who have not yet seen the light.  ","description":"","tags":null,"title":"Utils","uri":"/tht/utils/"},{"content":"Running Clone the THT repo.\ngit clone https://github.com/ethack/tht.git Then run THT in development mode.\ncd tht ./tht --dev This mounts certain files from this repository directly into the container which allows making changes to scripts without having to rebuild the image.\nBuilding Docker Image You can build the image manually with the following command. However, the Maxmind geo information will not be included.\ndocker build -t ethack/tht . If you want to build with Maxmind data, you’ll need to sign up for a free Maxmind license key. You can then specify your key when building.\ndocker build --build-arg MAXMIND_LICENSE=yourkeyhere -t ethack/tht . Building Documentation You’ll need Hugo. Official install instructions are here though the easiest way is to grab the latest Github release for your platform. Hugo is a single binary so there’s really not much to install.\nOnce you have Hugo you can compile and host the documentation locally with:\nhugo -s docs server -D Then access the local website http://localhost:1313 in your browser.\n Note:If you see a blank page or see warnings from hugo like found no layout file you need to download the theme, which you can do with: git submodule update --init\n ","description":"","tags":null,"title":"Development","uri":"/tht/development/"},{"content":"Overview One of the most common ways to transform data is to select certain columns to keep, while discarding the rest. This is exactly what chop does. You can accomplish the same thing with awk, cut, zeek-cut, jq, etc. but without changing your tool or syntax when your log format changes.\nHere’s a comparison between the different tools and log types they support.\n   Command Zeek TSV JSON CSV TSV Whitespace Custom     chop ✔️ ✔️ ✔️ ✔️ ✔️ ✔️   awk ✔️ †  ✔️ †† ✔️ †† ✔️ ✔️   cut ✔️ †  ✔️ †† ✔️ †† ✔️ ††† ✔️   zeek-cut ✔️        jq  ✔️       json-cut  ✔️       zq 'cut' ✔️ ✔️       mlr cut  ✔️ ✔️ ✔️ ✔️    xsv select   ✔️ ✔️  ✔️   tsv-select ✔️ †  ✔️ ✔️  ✔️     † returns junk from metadata †† will not handle quoted values containing delimeter ††† can pick space or tab but not both  Can you pre-process and post-process data and make most of these tools work? Sure. But why not let chop do it for you instead?\nThe best way to illustrate this is with examples.\nExamples Let’s say you want to pull out the destination IPs and ports from a Zeek TSV file. Here are some traditional ways you might do this.\nzeek-cut id.resp_h id.resp_p awk '{print $6,$7}' cut -d$'\\t' -f6,7 zq -f text 'cut id.resp_h,id.resp_p' sed -e '0,/^#fields\\t/s///' | grep -v '^#' | xsv select -d '\\t' id.resp_h,id.resp_p mlr --prepipe \"sed '0,/^#fields\\t/s///'\" --tsv --skip-comments cut -f id.resp_h,id.resp_p chop borrows its simplicity from zeek-cut to accomplish the same task.\nchop id.resp_h id.resp_p chop id.resp_h,id.resp_p If your logs don’t have headers (or you just prefer not to use them) you can specify field indexes instead.\nchop 6,7 chop 6 7 # a range will also work chop 6-7  Tipchop lets you specify fields with spaces, commas, or mix-and-match!\n Now let’s say you want to do the same thing (pull out the destination IPs and ports) but this time from a Zeek JSON file.\njson-cut id.resp_h id.resp_p zq -f text 'cut id.orig_h,id.resp_h,id.resp_p' jq -c '{\"id.resp_h\", \"id.resp_p\"}' mlr --json cut -f id.resp_h,id.resp_p  NoteSince JSON objects are sparse and un-ordered it doesn’t make sense to specify a field by its numerical index.\n Unless you were using zq you’ve already had to dust off a new tool and learn/remember its syntax. chop lets you use the same simple syntax you’ve already learned.\nchop id.resp_h id.resp_p Next, let’s say you want the src (1st column) and dst (3rd column) fields from a firewall log CSV file. Once again, starting with the traditional methods.\nawk -F, '{print $1,$3}' cut -d, -f1,3 xsv select src,dst xsv select 1,3 mlr --csv cut -f src,dst That’s not too bad. But you still have unnecessary command switches and boilerplate compared to chop.\nchop src dst chop 1,3 And what if those src and dst columns were whitespace separated?\nawk '{print $1,$3}' cut -d' ' -f1,3 # only if space separated, not both cut -d$'\\t' -f1,3 # only if tab separated, not both mlr --pprint cut -f src,dst By now, you’ve guessed it. chop’s syntax remains the same. Not only can you keep the same syntax between file formats, but you don’t have to think about which separators are used.\nchop src dst chop 1,3 Before chop, each scenario would require you to reach for a different tool and then figure out the syntax or transformations to get it working with your data. In the end, you’d have a command that’s cumbersome to type and unlikely to be used as-is for another log format.\nTipJust like with other tools, you can specify a custom single character as a delimeter with chop. E.g. chop -d'|' or chop -d':'.\n ","description":"Like `cut` but with column names and CSV/TSV/JSON/Zeek support without the boilerplate.\n","tags":null,"title":"Chop","uri":"/tht/filter/chop/"},{"content":"Overview The filter script is tailored for searching IP addresses and domains in Zeek logs. However, it is flexible enough that it can be used for other purpsoses as well.\nfilter has several special features. Most of these behaviors can be changed with flags, but by default it:\n Searches in the current directory tree for any conn.log files, including compressed files and rotated logs. Searches files in parallel using multiple cores. Uses faster alternatives to grep like ripgrep or ugrep, when available. Keeps Zeek TSV headers intact so that tools like zeek-cut can be used on the results. Requires all search terms to be found in the same line to cut down on piping to repeated searches. Escapes periods in search terms to prevent 10.0.1.0 from matching 10.0.100 or google.com from matching google1com.com. Adds boundaries around the search term to prevent google.com from matching fakegoogle.com.  Usage filter [--\u003clogtype\u003e] [OPTIONS] [search_term] [search_term...] [-- [OPTIONS]] Specify one or more [search_terms] to search either STDIN or log files. If you don't specify any search terms, all lines will be printed. --\u003clogtype\u003e is used to search logs of \"logtype\" (e.g. conn, dns, etc) in the current directory tree (default: conn) -|--stdin reads filenames to search from stdin instead -d|--dir \u003cdirglob\u003e will search logs in \u003cdirglob\u003e instead of current directory Lines must match all search terms by default. --or at least one search term is required to appear in a line (as opposed to all terms matching) Search terms will match on word boundaries by default. -s|--starts-with anchor search term to beginning of field (e.g. 192.168) -e|--ends-with anchor search term to end of field (e.g. google.com) -r|--regex signifies that [search_term(s)] should be treated as regexes -v|--invert-match will invert the matching -n|--dry-run print out the final search command rather than execute it You can specify search terms in other ways as well. -f|--file \u003cpatternfile.txt\u003e file containing newline separated search terms -p|--preset \u003cpreset\u003e use a common preset regex with current options being: rfc1918 matches private IPv4 addresses defined in RFC1918 ipv4 matches any IPv4 address filter will pick the best search tool for the situation. Use the following options to force a specific tool. --rg force use of ripgrep --ug force use of ugrep --zgrep force use of zgrep --cat force use of cat (useful for testing) Any arguments given after -- will be passed to the underlying search command. Comparison with Grep The best way to understand the benefits is with an example. filter is designed to be called as-is for the most common use cases. It would defeat its purpose if it required extra options every time.\n# find all conn log entries from the current directory tree containing 192.168.1.1 filter 192.168.1.1 Let’s look at what it would take to do this without using filter. A first attempt is straightforward:\ncat conn.*log | fgrep 192.168.1.1 # or zcat conn.*log.gz | fgrep 192.168.1.1 # or zfgrep 192.168.1.1 conn.*log* However, this has multiple issues:\n It is somewhat cumbersome to search both plaintext and gzip logs at once. Zeek TSV headers are stripped which means no piping to zeek-cut. grep doesn’t make use of multiple cores which increases the search time. Logs in subdirectories are not searched. Search will match 192.168.1.10, 192.168.1.19, 192.168.1.100, etc.  There are ways around each of these issues, but they add extra complexity to the command and more typing. You’d ultimately end up with something like this:\nfind . -regextype egrep -iregex '.*/conn\\b.*\\.log(.gz)?$' | xargs -n 1 -P $(nproc) zgrep -e '^#' -e '\\b192\\.168\\.1\\.1\\b' Which is roughly equivalent to the much shorter filter 192.168.1.1.\nExamples Let’s take a look at more examples.\nTipJust like traditional grep you can also pipe text to search as input.\n # log entries from stdin containing 192.168.1.1 cat conn.log | filter 192.168.1.1 # conn log entries containing 192.168; note: this could also match 10.10.192.168 filter 192.168 # conn log entries containing both 192.168.1.1 and 8.8.8.8 filter 192.168.1.1 8.8.8.8 # dns log entries containing 8.8.8.8 filter --dns 8.8.8.8 # dns log entries to Google or Cloudflare's DNS servers filter --dns --or 8.8.8.8 8.8.4.4 1.1.1.1 1.0.0.1 # http log entries containing google.com; note: this will also match google.com.fake.com filter --http google.com # conn JSON entries where the origin host is 192.168.1.1 filter -r '\"id.orig_h\":\"192.168.1.1\"' Where filter really shines is when you combine it with other tools that can parse Zeek logs, such as zeek-cut, conn-summary, and zq.\n# find all source IPs that queried evil.com filter --dns evil.com | zeek-cut id.orig_h | sort -V | uniq # find all IPs there were resolved by evil.com queries filter --dns evil.com | zeek-cut answers | filter -p ipv4 -- -o | sort -V | uniq # show a summary of traffic involving a specific IP address filter 1.2.3.4 | conn-summary # TODO zq example with a grouping You can also specify search terms inside a file. These search terms are given the same escaping treatment as if they were specified on the commandline.\nfilter --or -f patterns.txt If you’d like to restrict the search directory to something more than the current directory tree you can.\nfilter -d 2021-06-29 1.1.1.1 # globbing and bash brace expansion work as well (note: you'll need quotes around the argument) filter -d '2021-06-*' 1.1.1.1 filter -d '2021-06-{01..15}' 1.1.1.1 filter -d '2021-06-01' --dns 1.1.1.1 If you need to get even more granular in the files that filter searches you can pass them in on stdin by specifying -. The recommended method is to combine this with the fd command.\nfd conn 2021-06-* | filter - 1.1.1.1 fd http 2021-{01..06}-{01-31} | filter - google.com Performance TipUse filter to quickly reduce log volume before piping to more specialized tools.\n By design, filter will match the search string anywhere in the line. This means that if you want to search for an origin of 192.168.1.1, the best method is to first use filter and then another tool that can check a certain field such as awk, jq, zq.\n# TODO awk example # for JSON logs you can do something like this filter -r '\"id.orig_h\":\"192.168.1.1\"' # or this filter 192.168.1.1 | jq 'select(.\"id.orig_h\"==\"192.168.1.1\")' # or you can use zq for either type of Zeek log filter 192.168.1.1 | zq -f zeek \"id.orig_h = 192.168.1.1\" - You might be wondering in that last example why you would even need to use filter at all. You’d certainly get the same results by using the zq command on the unfiltered logs.\nIt comes down to performance. String searching tools like grep are going to be much faster than something like zq or jq that parses and interprets the field values in a log file. The benchmarks below show that it is quite a bit faster to first use filter to reduce log volume before doing more expensive matching, than it is to feed all the logs directly into zq or jq for field matching first.\nNoteSince filter uses multiple CPU cores, this also has the benefit of parallelizing the pipeline stage when the log volume is at its peak.\n $ hyperfine -w 1 \\  -n filter-then-select 'filter 10.55.182.100 | zq -f zeek \"id.orig_h = 10.55.182.100\" -' \\  -n cat-then-select 'zcat conn.* | zq -f zeek \"id.orig_h = 10.55.182.100\" -' Benchmark #1: filter-then-select Time (mean ± σ): 768.8 ms ± 31.5 ms [User: 3.399 s, System: 0.262 s] Range (min … max): 714.9 ms … 822.9 ms 10 runs Benchmark #2: cat-then-select Time (mean ± σ): 6.256 s ± 0.392 s [User: 11.595 s, System: 0.466 s] Range (min … max): 5.807 s … 7.240 s 10 runs Summary 'filter-then-select' ran 8.14 ± 0.61 times faster than 'cat-then-select' $ hyperfine -w 1 \\  -n cat-then-select 'cat conn.* | jq \"select(.\\\"id.orig_h\\\"==\\\"10.55.182.100\\\")\"' \\  -n filter-then-select 'filter 10.55.182.100 | jq \"select(.\\\"id.orig_h\\\"==\\\"10.55.182.100\\\")\"' Benchmark #1: cat-then-select Time (mean ± σ): 22.428 s ± 2.036 s [User: 22.185 s, System: 2.264 s] Range (min … max): 18.629 s … 24.577 s 10 runs Benchmark #2: filter-then-select Time (mean ± σ): 1.285 s ± 0.101 s [User: 2.455 s, System: 0.172 s] Range (min … max): 1.096 s … 1.386 s 10 runs Summary 'filter-then-select' ran 17.45 ± 2.09 times faster than 'cat-then-select' Advanced Usage Dry Run Sometimes it can be detrimental for a tool to hide the implementation details, especially when you’re unsure where an error lies. You can double-check the command that filter will run by using the -n or --dry-run flag. This will print out the command filter would have run without the flag.\n$ filter --dry-run 1.1.1.1 ug --no-filename --decompress --bool '^# OR (\\b1\\.1\\.1\\.1\\b)' ./conn.log | grep -v '^#'  TipAnother use of the --dry-run flag is as a command builder. Have filter output the complex command and you can use that as a starting place for further tweaking.\n You can also force filter to use a certain grep utility with a flag. See --help for all options.\n$ filter --grep --dry-run 1.1.1.1 echo ./conn.log | xargs -n 1 -P 12 zgrep -e '^#' -e '\\b1\\.1\\.1\\.1\\b' | grep -v '^#'  NoteNotice the grep -v '^# on the end of the commands printed above. If filter detects it is printing to a terminal it will remove lines beginning with #. To disable this behavior you just need to pipe to another program, such as cat. e.g. filter --dry-run 1.1.1.1 | cat\n Anchoring If you are searching for a partial IP address (e.g. “192.168”) or a partial domain (e.g. “google.com”) you may still get unintended matches. You can control this behavior with the --starts-with and --ends-with flags.\n# this will also match 10.10.192.168 or 10.192.168.10, etc. filter 192.168 # do this instead filter --starts-with 192.168 filter -s 192.168 # this will also match google.com.fake.com filter --http google.com # do this instead filter --http --ends-with google.com filter --http -e google.com Related Tools Here are some related tools that may be useful if filter doesn’t fit your use case.\n grepwide ripgrep ugrep grepcidr  FAQ Q: Why use filter over tools like awk, grep, jq, zq, etc.?\nA: filter complements or enhances many of these tools.\n For instance, using a regex search tool is nearly always faster than using awk, zq, or jq to match a line. By assuming a specific use case (searching Zeek logs for things like IP addresses) filter can automate a bunch of boilerplate like escaping periods in regexes, passing through Zeek headers, and recursively searching compressed files of one log type. grep on its own does not utilize parallel processing. This means either replacing it with an alternative or remembering the correct syntax to combine it with something like parallel or xargs.  ","description":"Parallel searching for IPs, domains, or regexes across [un]compressed logs. \nPlus special features for Zeek logs.\n","tags":null,"title":"Filter","uri":"/tht/filter/filter/"},{"content":"Zeek’s conn-summary.log file contains useful information about a network as a whole.\n\u003e== Total === 2018-03-11-23-58-55 - 2018-03-12-00-59-10 - Connections 20.0k - Payload 206.1m - Sampling 1.00% - Ports | Sources | Destinations | Services | Protocols | States | 53 66.0% | 10.55.200.10#1 39.0% | 172.16.200.11#2 18.0% | dns 66.0% | 17 66.0% | SF 78.5% | 443 29.5% | 10.55.200.11#3 23.5% | 165.227.88.15#4 3.0% | ssl 26.0% | 6 34.0% | S0 18.5% | 80 4.5% | 10.55.100.103#5 6.0% | 205.251.196.95#6 2.0% | http 7.5% | | RSTO 2.5% | | 10.55.100.100#7 5.0% | 172.217.4.70#8 2.0% | - 0.5% | | RSTR 0.5% | | 10.55.100.111#9 4.5% | 23.61.199.64#10 2.0% | | | | | 10.55.100.110#11 4.0% | 204.74.101.1#12 1.5% | | | | | 192.168.88.2#13 3.5% | 172.217.4.78#14 1.5% | | | | | 10.55.100.104#15 3.5% | 104.156.80.32#16 1.5% | | | | | 10.55.100.108#17 3.0% | 65.153.18.198#18 1.5% | | | | | 10.55.100.106#19 2.5% | 65.153.18.197#20 1.5% | | | | #1=\u003c???\u003e #2=\u003c???\u003e #3=\u003c???\u003e #4=\u003c???\u003e #5=\u003c???\u003e #6=ns-1119.awsdns-11.org #7=\u003c???\u003e #8=ord37s18-in-f6.1e100.net #9=\u003c???\u003e #10=a7-64.akam.net #11=\u003c???\u003e #12=udns2.ultradns.net #13=\u003c???\u003e #14=lga15s47-in-f78.1e100.net #15=\u003c???\u003e #16=\u003c???\u003e #17=\u003c???\u003e #18=\u003c???\u003e #19=\u003c???\u003e #20=\u003c???\u003e There’s a lot to unpack here. Each colum shows the top pieces of information, broken out by percentage of connections. For instance:\nPorts 53 66.0% 443 29.5% 80 4.5% This means that 66.0% of the connections were to port 53, 29.5% were to port 443, and the remaining 4.5% were to port 80. Note that this doesn’t specify TCP or UDP. However, the second-to-last column shows that protocol 17 (UDP) was 66.0% of the connections. This happens to match up with the percentage for port 53 so we know that UDP/53 was used.\nProtocols 17 66.0% 6 34.0% Similarly, the following columns break down the percentage of connections by both source and destination IP addresses.\nSources | Destinations 10.55.200.10#1 39.0% | 172.16.200.11#2 18.0% 10.55.200.11#3 23.5% | 165.227.88.15#4 3.0% 10.55.100.103#5 6.0% | 205.251.196.95#6 2.0% 10.55.100.100#7 5.0% | 172.217.4.70#8 2.0% 10.55.100.111#9 4.5% | 23.61.199.64#10 2.0% 10.55.100.110#11 4.0% | 204.74.101.1#12 1.5% 192.168.88.2#13 3.5% | 172.217.4.78#14 1.5% 10.55.100.104#15 3.5% | 104.156.80.32#16 1.5% 10.55.100.108#17 3.0% | 65.153.18.198#18 1.5% 10.55.100.106#19 2.5% | 65.153.18.197#20 1.5% The conn-summary.log file is generated by the trace-summary script. You can also run this script manually and access different options. The script has the ability to generate summaries from either live network data or from Zeek TSV or JSON-formatted conn.log files.\nTHT has a conn-summary script that is useful for generating summaries on the fly for many common use cases. It lets you pipe conn logs into the script. This means you can generate summaries based only on hosts or timeframes you care about.\nfilter 192.168.88.2 165.227.88.15 | conn-summary  InfoThe above example uses the filter script included with THT.\n conn-summary uses trace-summary under the hood, and the output is similar to what we saw above with the following differences:\n Internal to internal traffic is not included. That is, any connections between RFC1918 IP address ranges are excluded. Unlike the Zeek conn-summary.log the data is not sampled by default. This means the statistics presented include all the connections instead of a sample size. The percentages are output twice: once for connections and once for bytes. That means you can see both where the majority of connections went in the top table and where the majority of the data transferred went in the bottom table. The Zeek conn-summary.log includes a bunch of other breakdowns based on different IP address ranges that are not included in the output here.  Connections: ============ \u003e== Total === 2018-01-30-12-14-02 - 2018-01-31-12-14-00 - Connections 239.4k - Payload 35.4m - Ports | Sources | Destinations | Services | Protocols | States | 53 98.8% | 192.168.88.2 100.0% | 165.227.88.15 90.9% | dns 98.8% | 17 100.0% | SF 99.8% | 123 1.2% | 165.227.88.15 0.0% | 208.84.2.53 0.5% | ntp 1.2% | 6 0.0% | S0 0.1% | 443 0.0% | | 208.76.45.53 0.5% | ssl 0.0% | 1 0.0% | REJ 0.0% | 3 0.0% | | 95.101.36.192 0.2% | - 0.0% | | OTH 0.0% | | | 23.211.133.192 0.1% | | | | | | 95.100.173.192 0.1% | | | | | | 95.100.168.194 0.1% | | | | | | 184.85.248.194 0.1% | | | | | | 96.7.49.194 0.1% | | | | | | 208.109.255.2 0.1% | | | | Bytes: ====== \u003e== Total === 2018-01-30-12-14-02 - 2018-01-31-12-14-00 - Connections 239.4k - Payload 35.4m - Ports | Sources | Destinations | Services | Protocols | States | 53 98.3% | 192.168.88.2 100.0% | 165.227.88.15 88.2% | dns 98.3% | 17 99.1% | SF 100.0% | 443 0.9% | 165.227.88.15 0.0% | 162.208.119.40 0.9% | ssl 0.9% | 6 0.9% | OTH 0.0% | 123 0.8% | | 208.76.45.53 0.4% | ntp 0.8% | 1 0.0% | S0 0.0% | 3 0.0% | | 208.84.2.53 0.4% | - 0.0% | | REJ 0.0% | | | 95.101.36.192 0.2% | | | | | | 23.211.133.192 0.2% | | | | | | 95.100.173.192 0.2% | | | | | | 184.85.248.194 0.2% | | | | | | 184.26.161.192 0.2% | | | | | | 95.100.168.194 0.2% | | | | Alternatives  https://github.com/jbaggs/conn-summary - reads Zeek logs from Elasticsearch  ","description":"Output a summary of connections given an input of Zeek logs. Great for getting an overview of a subset of logs.\n","tags":null,"title":"Conn Summary","uri":"/tht/summarize/conn-summary/"},{"content":"","description":"","tags":null,"title":"Categories","uri":"/tht/categories/"},{"content":"","description":"","tags":null,"title":"Tags","uri":"/tht/tags/"}]